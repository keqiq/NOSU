{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# import sys\n",
    "# from pathlib import Path\n",
    "# sys.path.append(str(Path.cwd().parent))\n",
    "# from utils.DatasetGenerator import get_X, get_y, get_pos_sequences, read_sequences\n",
    "# path = '../Data/train_pos/annoy'\n",
    "# df_y_pos, hr = get_y(path, 'pos')\n",
    "# df_X = get_X(path, hr)\n",
    "# get_pos_sequences([df_X, df_y_pos, path], 10, 500)\n",
    "# input_seq, target_seq, target_obj = read_sequences(path, 'pos')\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # debug\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# import sys\n",
    "# from pathlib import Path\n",
    "# sys.path.append(str(Path.cwd().parent))\n",
    "# from utils.Visualizer import visualize\n",
    "# from utils.DataParser import parse_map, parse_replay_pos\n",
    "# path = '../Data/train_pos/shink/'\n",
    "# replay_path = list(Path(path).glob('*.osr'))[0]\n",
    "# map_path = list(Path(path).glob('*.osu'))[0]\n",
    "# replay_data, hr = parse_replay_pos(replay_path)\n",
    "# hit_objects, _ = parse_map(map_path, hr)\n",
    "# song = list(Path(path).glob('audio.*'))[0]\n",
    "# visualize(hit_objects, replay_data, song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "%load_ext autoreload\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "data_paths = {\n",
    "    'train': '../Data/train_pos',\n",
    "    'valid': '../Data/valid_pos'\n",
    "}\n",
    "REGENERATE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting train data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:04<00:00,  7.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting valid data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.90it/s]\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "# Data parsing\n",
    "from utils.DatasetGenerator import get_set\n",
    "datasets = {\n",
    "    'train': None,\n",
    "    'valid': None\n",
    "}\n",
    "\n",
    "for idx, set in enumerate(data_paths):\n",
    "    print(f'Converting {set} data')\n",
    "    data = get_set(data_paths[set], 'pos', REGENERATE)\n",
    "    \n",
    "    datasets[set] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating train sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:14<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating valid sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:03<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving train sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:51<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving valid sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:04<00:00,  1.51s/it]\n"
     ]
    }
   ],
   "source": [
    "# Sequence generation\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\".*weights_only=False.*\")\n",
    "from utils.DatasetGenerator import create_set_sequences, get_set_sequences\n",
    "from itertools import chain\n",
    "\n",
    "N = 5\n",
    "window = 450\n",
    "for idx, set in enumerate(datasets):\n",
    "    if len(datasets[set]) > 0:\n",
    "        print(f'Generating {set} sequences')\n",
    "        create_set_sequences(datasets[set], N, window, 'pos')\n",
    "\n",
    "combined_train_pos_input = None\n",
    "combined_train_pos_target = None\n",
    "combined_train_pos_object = None\n",
    "valid_sequences = None\n",
    "for idx, set in enumerate(data_paths):\n",
    "    print(f'Retrieving {set} sequences')\n",
    "    pos_input_sequences, pos_target_sequences, pos_target_objects = get_set_sequences(data_paths[set], 'pos')\n",
    "    if set == 'train':\n",
    "        combined_train_pos_input = list(chain.from_iterable(pos_input_sequences))\n",
    "        combined_train_pos_target = list(chain.from_iterable(pos_target_sequences))\n",
    "        combined_train_pos_object = list(chain.from_iterable(pos_target_objects))\n",
    "    else:\n",
    "        valid_sequences = [pos_input_sequences, pos_target_sequences, pos_target_objects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from Model.OSUDataset import OsuDataset, pos_collate_fn\n",
    "data_paths = {\n",
    "    'train': '../Data/train_pos',\n",
    "    'valid': '../Data/valid_pos'\n",
    "}\n",
    "BATCH_SIZE_TRAIN = 1024\n",
    "if os.path.exists('./pos_dataset/train_dataset.pth') and not REGENERATE:\n",
    "    train_dataset = torch.load('./pos_dataset/train_dataset.pth')\n",
    "else:\n",
    "    train_dataset = OsuDataset(combined_train_pos_input, combined_train_pos_target, combined_train_pos_object)\n",
    "    torch.save(train_dataset, './pos_dataset/train_dataset.pth')\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE_TRAIN, shuffle=True, drop_last=True, collate_fn=pos_collate_fn, pin_memory=True)\n",
    "\n",
    "BATCH_SIZE_VALID = 64\n",
    "valid_loaders = []\n",
    "valid_folders = os.listdir(data_paths['valid'])\n",
    "valid_data_size = sum(os.path.isdir(os.path.join(data_paths['valid'], folder)) for folder in valid_folders)\n",
    "for idx in range(valid_data_size):\n",
    "    if os.path.exists(f'./pos_dataset/valid_dataset_{idx}.pth') and not REGENERATE:\n",
    "        valid_dataset = torch.load(f'./pos_dataset/valid_dataset_{idx}.pth')\n",
    "    else:\n",
    "        valid_dataset = valid_dataset = OsuDataset(valid_sequences[0][idx], valid_sequences[1][idx], valid_sequences[2][idx])\n",
    "        torch.save(valid_dataset, f'./pos_dataset/valid_dataset_{idx}.pth')\n",
    "        \n",
    "    valid_loader =  DataLoader(valid_dataset, batch_size=BATCH_SIZE_VALID, shuffle=False, drop_last=True, collate_fn=pos_collate_fn, pin_memory=True)\n",
    "    valid_loaders.append(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from Model.OSUModel import PositionEncoder, PositionDecoder, OSUModelPos\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "pos_input_size = 12\n",
    "\n",
    "pos_hidden_size = 24\n",
    "pos_num_layers = 2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Encoders take (input_size, hidden_size, num_layers)\n",
    "pos_encoder = PositionEncoder(pos_input_size, pos_hidden_size, pos_num_layers).to(device)\n",
    "\n",
    "# Decoders takes (hidden_size, num_layers)\n",
    "pos_decoder = PositionDecoder(pos_hidden_size, pos_num_layers).to(device)\n",
    "\n",
    "osuModelPos = OSUModelPos(pos_encoder, pos_decoder, device).to(device)\n",
    "\n",
    "pos_criterion = nn.L1Loss(reduction='none')\n",
    "\n",
    "pos_optimizer = torch.optim.Adam(\n",
    "    osuModelPos.parameters(),\n",
    "    lr = 0.01,\n",
    "    weight_decay=1e-5\n",
    ")\n",
    "\n",
    "\n",
    "pos_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(pos_optimizer, mode='min',\n",
    "                                                    factor=0.1, patience=15,\n",
    "                                                    threshold=1e-4)\n",
    "\n",
    "  \n",
    "def get_pos_loss(pos_outputs, targets):\n",
    "    # Position targets (batch_size, 2, 2)\n",
    "    # Removing previous positions from targets\n",
    "    pos_targets = targets[:, 1:, :]  \n",
    "\n",
    "    # Position loss with absolute error\n",
    "    pos_loss = pos_criterion(pos_outputs, pos_targets)\n",
    "    \n",
    "    # Sum the loss along the last dimension and compute the mean over the batch\n",
    "    pos_loss = pos_loss.sum(dim=2).mean()\n",
    "\n",
    "    return pos_loss\n",
    "\n",
    "def get_hybrid_loss(pos_outputs, targets, train_objects):\n",
    "    # Position targets (batch_size, 2, 2)\n",
    "    # Removing previous positions from targets\n",
    "    pos_targets = targets[:, 1:, :]  # Shape: (batch_size, 1, 2)\n",
    "    \n",
    "    # Position loss with absolute error\n",
    "    pos_loss = pos_criterion(pos_outputs, pos_targets)  # Shape: (batch_size, 1, 2)\n",
    "    \n",
    "    # Sum the loss along the last dimension and compute the mean over the batch\n",
    "    pos_loss = pos_loss.sum(dim=2).mean()\n",
    "    \n",
    "    # Object position (batch_size, 2)\n",
    "    object_pos = train_objects[:, :2]  # Shape: (batch_size, 2)\n",
    "    object_pos = object_pos.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
    "    \n",
    "    # Time values (batch_size,)\n",
    "    time_values = torch.abs(train_objects[:, 2])\n",
    "    \n",
    "    # Valid types (hit circles or slider start)\n",
    "    valid = train_objects[:, 3]\n",
    "    \n",
    "    # Compute weights: weight increases from 0 to 1 as time decreases from 0.05 to 0\n",
    "    weights = torch.clamp((0.05 - time_values) / 0.05, min=0, max=1) * 3  # Shape: (batch_size,)\n",
    "    \n",
    "    # Compute loss between pos_outputs and object_pos\n",
    "    object_loss = pos_criterion(pos_outputs, object_pos)  # Shape: (batch_size, 1, 2)\n",
    "    object_loss = object_loss.sum(dim=2).squeeze(1)  # Shape: (batch_size,)\n",
    "    \n",
    "    # Apply weights\n",
    "    weighted_object_loss = object_loss * weights * valid  # Shape: (batch_size,)\n",
    "    \n",
    "    # Compute mean over the batch\n",
    "    object_loss_mean = weighted_object_loss.mean()\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = pos_loss + object_loss_mean\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000]T Loss Pos: 0.1038 | V Loss Pos: 0.0503 | LR Pos: 0.01 | \n",
      "Epoch [2/1000]T Loss Pos: 0.0499 | V Loss Pos: 0.0507 | LR Pos: 0.01 | \n",
      "Epoch [3/1000]T Loss Pos: 0.0428 | V Loss Pos: 0.0317 | LR Pos: 0.01 | \n",
      "Epoch [4/1000]T Loss Pos: 0.0404 | V Loss Pos: 0.0255 | LR Pos: 0.01 | \n",
      "Epoch [5/1000]T Loss Pos: 0.0375 | V Loss Pos: 0.0262 | LR Pos: 0.01 | \n",
      "Epoch [6/1000]T Loss Pos: 0.0340 | V Loss Pos: 0.0251 | LR Pos: 0.01 | \n",
      "Epoch [7/1000]T Loss Pos: 0.0356 | V Loss Pos: 0.0329 | LR Pos: 0.01 | \n",
      "Epoch [8/1000]T Loss Pos: 0.0323 | V Loss Pos: 0.0275 | LR Pos: 0.01 | \n",
      "Epoch [9/1000]T Loss Pos: 0.0340 | V Loss Pos: 0.0257 | LR Pos: 0.01 | \n",
      "Epoch [10/1000]T Loss Pos: 0.0330 | V Loss Pos: 0.0246 | LR Pos: 0.01 | \n",
      "Epoch [11/1000]T Loss Pos: 0.0324 | V Loss Pos: 0.0250 | LR Pos: 0.01 | \n",
      "Epoch [12/1000]T Loss Pos: 0.0321 | V Loss Pos: 0.0242 | LR Pos: 0.01 | \n",
      "Epoch [13/1000]T Loss Pos: 0.0336 | V Loss Pos: 0.0250 | LR Pos: 0.01 | \n",
      "Epoch [14/1000]T Loss Pos: 0.0312 | V Loss Pos: 0.0256 | LR Pos: 0.01 | \n",
      "Epoch [15/1000]T Loss Pos: 0.0315 | V Loss Pos: 0.0246 | LR Pos: 0.01 | \n",
      "Epoch [16/1000]T Loss Pos: 0.0316 | V Loss Pos: 0.0285 | LR Pos: 0.01 | \n",
      "Epoch [17/1000]T Loss Pos: 0.0317 | V Loss Pos: 0.0248 | LR Pos: 0.01 | \n",
      "Epoch [18/1000]T Loss Pos: 0.0308 | V Loss Pos: 0.0288 | LR Pos: 0.01 | \n",
      "Epoch [19/1000]T Loss Pos: 0.0312 | V Loss Pos: 0.0238 | LR Pos: 0.01 | \n",
      "Epoch [20/1000]T Loss Pos: 0.0312 | V Loss Pos: 0.0250 | LR Pos: 0.01 | \n",
      "Epoch [21/1000]T Loss Pos: 0.0312 | V Loss Pos: 0.0239 | LR Pos: 0.01 | \n",
      "Epoch [22/1000]T Loss Pos: 0.0311 | V Loss Pos: 0.0245 | LR Pos: 0.01 | \n",
      "Epoch [23/1000]T Loss Pos: 0.0300 | V Loss Pos: 0.0244 | LR Pos: 0.01 | \n",
      "Epoch [24/1000]T Loss Pos: 0.0299 | V Loss Pos: 0.0252 | LR Pos: 0.01 | \n",
      "Epoch [25/1000]T Loss Pos: 0.0309 | V Loss Pos: 0.0265 | LR Pos: 0.01 | \n",
      "Epoch [26/1000]T Loss Pos: 0.0300 | V Loss Pos: 0.0249 | LR Pos: 0.01 | \n",
      "Epoch [27/1000]T Loss Pos: 0.0305 | V Loss Pos: 0.0258 | LR Pos: 0.01 | \n",
      "Epoch [28/1000]T Loss Pos: 0.0302 | V Loss Pos: 0.0241 | LR Pos: 0.01 | \n",
      "Epoch [29/1000]T Loss Pos: 0.0303 | V Loss Pos: 0.0244 | LR Pos: 0.01 | \n",
      "Epoch [30/1000]T Loss Pos: 0.0302 | V Loss Pos: 0.0246 | LR Pos: 0.01 | \n",
      "Epoch [31/1000]T Loss Pos: 0.0298 | V Loss Pos: 0.0249 | LR Pos: 0.01 | \n",
      "Epoch [32/1000]T Loss Pos: 0.0297 | V Loss Pos: 0.0241 | LR Pos: 0.01 | \n",
      "Epoch [33/1000]T Loss Pos: 0.0296 | V Loss Pos: 0.0257 | LR Pos: 0.01 | \n",
      "Epoch [34/1000]T Loss Pos: 0.0291 | V Loss Pos: 0.0238 | LR Pos: 0.01 | \n",
      "Epoch [35/1000]T Loss Pos: 0.0290 | V Loss Pos: 0.0248 | LR Pos: 0.01 | \n",
      "Epoch [36/1000]T Loss Pos: 0.0285 | V Loss Pos: 0.0242 | LR Pos: 0.01 | \n",
      "Epoch [37/1000]T Loss Pos: 0.0282 | V Loss Pos: 0.0251 | LR Pos: 0.01 | \n",
      "Epoch [38/1000]T Loss Pos: 0.0279 | V Loss Pos: 0.0256 | LR Pos: 0.01 | \n",
      "Epoch [39/1000]T Loss Pos: 0.0280 | V Loss Pos: 0.0254 | LR Pos: 0.01 | \n",
      "Epoch [40/1000]T Loss Pos: 0.0285 | V Loss Pos: 0.0249 | LR Pos: 0.01 | \n",
      "Epoch [41/1000]T Loss Pos: 0.0276 | V Loss Pos: 0.0254 | LR Pos: 0.01 | \n",
      "Epoch [42/1000]T Loss Pos: 0.0274 | V Loss Pos: 0.0260 | LR Pos: 0.01 | \n",
      "Epoch [43/1000]T Loss Pos: 0.0279 | V Loss Pos: 0.0271 | LR Pos: 0.01 | \n",
      "Epoch [44/1000]T Loss Pos: 0.0273 | V Loss Pos: 0.0252 | LR Pos: 0.01 | \n",
      "Epoch [45/1000]T Loss Pos: 0.0271 | V Loss Pos: 0.0251 | LR Pos: 0.01 | \n",
      "Epoch [46/1000]T Loss Pos: 0.0270 | V Loss Pos: 0.0251 | LR Pos: 0.01 | \n",
      "Epoch [47/1000]T Loss Pos: 0.0273 | V Loss Pos: 0.0260 | LR Pos: 0.01 | \n",
      "Epoch [48/1000]T Loss Pos: 0.0267 | V Loss Pos: 0.0253 | LR Pos: 0.01 | \n",
      "Epoch [49/1000]T Loss Pos: 0.0270 | V Loss Pos: 0.0261 | LR Pos: 0.01 | \n",
      "Epoch [50/1000]T Loss Pos: 0.0266 | V Loss Pos: 0.0264 | LR Pos: 0.001 | \n",
      "Epoch [51/1000]T Loss Pos: 0.0243 | V Loss Pos: 0.0256 | LR Pos: 0.001 | \n",
      "Epoch [52/1000]T Loss Pos: 0.0241 | V Loss Pos: 0.0254 | LR Pos: 0.001 | \n",
      "Epoch [53/1000]T Loss Pos: 0.0240 | V Loss Pos: 0.0255 | LR Pos: 0.001 | \n",
      "Epoch [54/1000]T Loss Pos: 0.0240 | V Loss Pos: 0.0260 | LR Pos: 0.001 | \n",
      "Epoch [55/1000]T Loss Pos: 0.0239 | V Loss Pos: 0.0255 | LR Pos: 0.001 | \n",
      "Epoch [56/1000]T Loss Pos: 0.0239 | V Loss Pos: 0.0259 | LR Pos: 0.001 | \n",
      "Epoch [57/1000]T Loss Pos: 0.0239 | V Loss Pos: 0.0255 | LR Pos: 0.001 | \n",
      "Epoch [58/1000]T Loss Pos: 0.0238 | V Loss Pos: 0.0253 | LR Pos: 0.001 | \n",
      "Epoch [59/1000]T Loss Pos: 0.0238 | V Loss Pos: 0.0255 | LR Pos: 0.001 | \n",
      "Epoch [60/1000]T Loss Pos: 0.0238 | V Loss Pos: 0.0258 | LR Pos: 0.001 | \n",
      "Epoch [61/1000]T Loss Pos: 0.0237 | V Loss Pos: 0.0255 | LR Pos: 0.001 | \n",
      "Epoch [62/1000]T Loss Pos: 0.0237 | V Loss Pos: 0.0256 | LR Pos: 0.001 | \n",
      "Epoch [63/1000]T Loss Pos: 0.0237 | V Loss Pos: 0.0259 | LR Pos: 0.001 | \n",
      "Epoch [64/1000]T Loss Pos: 0.0237 | V Loss Pos: 0.0256 | LR Pos: 0.001 | \n",
      "Epoch [65/1000]T Loss Pos: 0.0237 | V Loss Pos: 0.0253 | LR Pos: 0.001 | \n",
      "Epoch [66/1000]T Loss Pos: 0.0236 | V Loss Pos: 0.0254 | LR Pos: 0.0001 | \n",
      "Epoch [67/1000]T Loss Pos: 0.0233 | V Loss Pos: 0.0255 | LR Pos: 0.0001 | \n",
      "Epoch [68/1000]T Loss Pos: 0.0233 | V Loss Pos: 0.0255 | LR Pos: 0.0001 | \n",
      "Epoch [69/1000]T Loss Pos: 0.0233 | V Loss Pos: 0.0254 | LR Pos: 0.0001 | \n",
      "Epoch [70/1000]T Loss Pos: 0.0233 | V Loss Pos: 0.0256 | LR Pos: 0.0001 | \n",
      "Epoch [71/1000]T Loss Pos: 0.0233 | V Loss Pos: 0.0255 | LR Pos: 0.0001 | \n",
      "Epoch [72/1000]T Loss Pos: 0.0233 | V Loss Pos: 0.0254 | LR Pos: 0.0001 | \n",
      "Epoch [73/1000]T Loss Pos: 0.0233 | V Loss Pos: 0.0255 | LR Pos: 0.0001 | \n",
      "Epoch [74/1000]T Loss Pos: 0.0233 | V Loss Pos: 0.0254 | LR Pos: 0.0001 | \n",
      "Epoch [75/1000]T Loss Pos: 0.0233 | V Loss Pos: 0.0254 | LR Pos: 0.0001 | \n",
      "Epoch [76/1000]T Loss Pos: 0.0233 | V Loss Pos: 0.0254 | LR Pos: 0.0001 | \n",
      "Epoch [77/1000]T Loss Pos: 0.0233 | V Loss Pos: 0.0253 | LR Pos: 0.0001 | \n",
      "Epoch [78/1000]T Loss Pos: 0.0233 | V Loss Pos: 0.0254 | LR Pos: 0.0001 | \n",
      "Epoch [79/1000]T Loss Pos: 0.0233 | V Loss Pos: 0.0254 | LR Pos: 0.0001 | \n",
      "Epoch [80/1000]T Loss Pos: 0.0233 | V Loss Pos: 0.0256 | LR Pos: 0.0001 | \n",
      "Epoch [81/1000]T Loss Pos: 0.0233 | V Loss Pos: 0.0256 | LR Pos: 0.0001 | \n",
      "Epoch [82/1000]T Loss Pos: 0.0233 | V Loss Pos: 0.0254 | LR Pos: 1e-05 | \n",
      "Epoch [83/1000]T Loss Pos: 0.0232 | V Loss Pos: 0.0255 | LR Pos: 1e-05 | \n",
      "Epoch [84/1000]T Loss Pos: 0.0232 | V Loss Pos: 0.0255 | LR Pos: 1e-05 | \n",
      "Epoch [85/1000]T Loss Pos: 0.0232 | V Loss Pos: 0.0255 | LR Pos: 1e-05 | \n",
      "Epoch [86/1000]T Loss Pos: 0.0232 | V Loss Pos: 0.0255 | LR Pos: 1e-05 | \n",
      "Epoch [87/1000]T Loss Pos: 0.0232 | V Loss Pos: 0.0255 | LR Pos: 1e-05 | \n",
      "Epoch [88/1000]T Loss Pos: 0.0232 | V Loss Pos: 0.0255 | LR Pos: 1e-05 | \n",
      "Epoch [89/1000]T Loss Pos: 0.0232 | V Loss Pos: 0.0255 | LR Pos: 1e-05 | \n",
      "Epoch [90/1000]T Loss Pos: 0.0232 | V Loss Pos: 0.0255 | LR Pos: 1e-05 | \n",
      "Epoch [91/1000]T Loss Pos: 0.0232 | V Loss Pos: 0.0255 | LR Pos: 1e-05 | \n",
      "Epoch [92/1000]T Loss Pos: 0.0232 | V Loss Pos: 0.0255 | LR Pos: 1e-05 | \n",
      "Epoch [93/1000]T Loss Pos: 0.0232 | V Loss Pos: 0.0255 | LR Pos: 1e-05 | \n",
      "Epoch [94/1000]T Loss Pos: 0.0232 | V Loss Pos: 0.0255 | LR Pos: 1e-05 | \n",
      "Epoch [95/1000]T Loss Pos: 0.0232 | V Loss Pos: 0.0255 | LR Pos: 1e-05 | \n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 1000\n",
    "total_train_len = len(train_loader)\n",
    "total_valid_len = 0\n",
    "for valid_loader in valid_loaders:\n",
    "    total_valid_len += len(valid_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    osuModelPos.train()\n",
    "    train_loss_pos = 0\n",
    "    \n",
    "    # Training\n",
    "    # for train_inputs, train_targets, input_lengths, target_lengths in train_loader:\n",
    "    for train_inputs, train_targets, input_lengths, train_objects in train_loader:\n",
    "        train_inputs, train_targets, train_objects = train_inputs.to(device), train_targets.to(device), train_objects.to(device)\n",
    "        \n",
    "        pos_optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagate\n",
    "        train_pos_outputs = osuModelPos(train_inputs, train_targets)\n",
    "        \n",
    "        # Compute losses\n",
    "        loss_pos = get_hybrid_loss(train_pos_outputs, train_targets, train_objects)\n",
    "        # loss_pos = get_pos_loss(train_pos_outputs, train_targets)\n",
    "        \n",
    "        # Accumulate losses\n",
    "        train_loss_pos += loss_pos.item()\n",
    "        \n",
    "        # Position encoder decoder back propagation\n",
    "        loss_pos.backward()\n",
    "        \n",
    "        # Gradient clipping \n",
    "        torch.nn.utils.clip_grad_norm_(osuModelPos.parameters(), max_norm=1.0)\n",
    "        # Updating weights\n",
    "        pos_optimizer.step()\n",
    "        \n",
    "    train_loss_pos /= total_train_len\n",
    "    \n",
    "    # Validation\n",
    "    osuModelPos.eval()\n",
    "    total_valid_loss_pos = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for valid_loader in valid_loaders:\n",
    "            valid_loss_pos = 0\n",
    "            valid_loss_key = 0\n",
    "            # for valid_inputs, valid_targets, input_lengths, target_lengths in valid_loader:\n",
    "            for valid_inputs, valid_targets, input_lengths, valid_objects in valid_loader:\n",
    "                valid_inputs, valid_targets = valid_inputs.to(device), valid_targets.to(device)\n",
    "\n",
    "                valid_pos_outputs = osuModelPos(valid_inputs, valid_targets)\n",
    "                \n",
    "                # loss_pos = get_weighted_pos_loss(valid_pos_outputs, valid_targets, valid_inputs)\n",
    "                loss_pos = get_pos_loss(valid_pos_outputs, valid_targets)\n",
    "\n",
    "                valid_loss_pos += loss_pos.item()\n",
    "                \n",
    "            total_valid_loss_pos += valid_loss_pos\n",
    "            \n",
    "    total_valid_loss_pos /= total_valid_len\n",
    "    \n",
    "    pos_scheduler.step(total_valid_loss_pos)\n",
    "    \n",
    "    pos_lr = pos_optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Printing info\n",
    "    print(\n",
    "    f\"Epoch [{epoch+1}/{num_epochs}]\"\n",
    "    f\"T Loss Pos: {train_loss_pos:.4f} | \"\n",
    "    f\"V Loss Pos: {total_valid_loss_pos:.4f} | \"\n",
    "    f\"LR Pos: {pos_lr} | \"\n",
    "    # f\"TFR: {teacher_forcing_ratio:.2f}\"\n",
    "    )\n",
    "    \n",
    "    if pos_lr < 1e-5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = '0.9'\n",
    "torch.save(osuModelPos.state_dict(), f'./OSU_model_pos_{VERSION}.pth')     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "osu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
