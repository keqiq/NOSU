{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "%load_ext autoreload\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "data_paths = {\n",
    "    'train': '../Data/train_pos',\n",
    "    'valid': '../Data/valid_pos'\n",
    "}\n",
    "REGENERATE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting train data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:06<00:00,  5.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting valid data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.89it/s]\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "# Data parsing\n",
    "from utils.DatasetGenerator import get_set\n",
    "datasets = {\n",
    "    'train': None,\n",
    "    'valid': None\n",
    "}\n",
    "\n",
    "for idx, set in enumerate(data_paths):\n",
    "    print(f'Converting {set} data')\n",
    "    data = get_set(data_paths[set], 'pos', REGENERATE)\n",
    "    \n",
    "    datasets[set] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating train sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:16<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating valid sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:03<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving train sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:51<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving valid sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:04<00:00,  1.64s/it]\n"
     ]
    }
   ],
   "source": [
    "# Sequence generation\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\".*weights_only=False.*\")\n",
    "from utils.DatasetGenerator import create_set_sequences, get_set_sequences\n",
    "from itertools import chain\n",
    "\n",
    "N = 5\n",
    "window = 500\n",
    "for idx, set in enumerate(datasets):\n",
    "    if len(datasets[set]) > 0:\n",
    "        print(f'Generating {set} sequences')\n",
    "        create_set_sequences(datasets[set], N, window, 'pos')\n",
    "\n",
    "combined_train_pos_input = None\n",
    "combined_train_pos_target = None\n",
    "combined_train_pos_object = None\n",
    "valid_sequences = None\n",
    "for idx, set in enumerate(data_paths):\n",
    "    print(f'Retrieving {set} sequences')\n",
    "    pos_input_sequences, pos_target_sequences, pos_target_objects = get_set_sequences(data_paths[set], 'pos')\n",
    "    if set == 'train':\n",
    "        combined_train_pos_input = list(chain.from_iterable(pos_input_sequences))\n",
    "        combined_train_pos_target = list(chain.from_iterable(pos_target_sequences))\n",
    "        combined_train_pos_object = list(chain.from_iterable(pos_target_objects))\n",
    "    else:\n",
    "        valid_sequences = [pos_input_sequences, pos_target_sequences, pos_target_objects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from Model.OSUDataset import OsuDataset, pos_collate_fn\n",
    "data_paths = {\n",
    "    'train': '../Data/train_pos',\n",
    "    'valid': '../Data/valid_pos'\n",
    "}\n",
    "BATCH_SIZE_TRAIN = 1024\n",
    "if os.path.exists('./pos_dataset/train_dataset.pth') and not REGENERATE:\n",
    "    train_dataset = torch.load('./pos_dataset/train_dataset.pth')\n",
    "else:\n",
    "    train_dataset = OsuDataset(combined_train_pos_input, combined_train_pos_target, combined_train_pos_object)\n",
    "    torch.save(train_dataset, './pos_dataset/train_dataset.pth')\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE_TRAIN, shuffle=True, drop_last=True, collate_fn=pos_collate_fn, pin_memory=True)\n",
    "\n",
    "BATCH_SIZE_VALID = 64\n",
    "valid_loaders = []\n",
    "valid_folders = os.listdir(data_paths['valid'])\n",
    "valid_data_size = sum(os.path.isdir(os.path.join(data_paths['valid'], folder)) for folder in valid_folders)\n",
    "for idx in range(valid_data_size):\n",
    "    if os.path.exists(f'./pos_dataset/valid_dataset_{idx}.pth') and not REGENERATE:\n",
    "        valid_dataset = torch.load(f'./pos_dataset/valid_dataset_{idx}.pth')\n",
    "    else:\n",
    "        valid_dataset = valid_dataset = OsuDataset(valid_sequences[0][idx], valid_sequences[1][idx], valid_sequences[2][idx])\n",
    "        torch.save(valid_dataset, f'./pos_dataset/valid_dataset_{idx}.pth')\n",
    "        \n",
    "    valid_loader =  DataLoader(valid_dataset, batch_size=BATCH_SIZE_VALID, shuffle=False, drop_last=True, collate_fn=pos_collate_fn, pin_memory=True)\n",
    "    valid_loaders.append(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from Model.OSUModel import PositionEncoder, PositionDecoder, OSUModelPos\n",
    "pos_input_size = 12\n",
    "\n",
    "pos_hidden_size = 24\n",
    "pos_num_layers = 2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Encoders take (input_size, hidden_size, num_layers)\n",
    "pos_encoder = PositionEncoder(pos_input_size, pos_hidden_size, pos_num_layers).to(device)\n",
    "\n",
    "# Decoders takes (hidden_size, num_layers)\n",
    "pos_decoder = PositionDecoder(pos_hidden_size, pos_num_layers).to(device)\n",
    "\n",
    "osuModelPos = OSUModelPos(pos_encoder, pos_decoder, device).to(device)\n",
    "\n",
    "# Absolute loss currently the only loss which give good results and i've tried many\n",
    "pos_criterion = nn.L1Loss(reduction='none')\n",
    "\n",
    "pos_optimizer = torch.optim.Adam(\n",
    "    osuModelPos.parameters(),\n",
    "    lr = 0.01,\n",
    "    weight_decay=1e-5\n",
    ")\n",
    "\n",
    "pos_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(pos_optimizer, mode='min',\n",
    "                                                    factor=0.1, patience=15,\n",
    "                                                    threshold=1e-4)\n",
    "  \n",
    "def get_pos_loss(pos_outputs, targets):\n",
    "    # Position targets (batch_size, 2, 2)\n",
    "    # Removing previous positions from targets\n",
    "    pos_targets = targets[:, 1:, :]  \n",
    "\n",
    "    # Position loss with absolute error\n",
    "    pos_loss = pos_criterion(pos_outputs, pos_targets)\n",
    "    \n",
    "    # Sum the loss along the last dimension and compute the mean over the batch\n",
    "    pos_loss = pos_loss.sum(dim=2).mean()\n",
    "\n",
    "    return pos_loss\n",
    "\n",
    "def get_hybrid_loss(pos_outputs, targets, train_objects):\n",
    "    # Position targets (batch_size, 2, 2)\n",
    "    # Removing previous positions from targets\n",
    "    pos_targets = targets[:, 1:, :]  # Shape: (batch_size, 1, 2)\n",
    "    \n",
    "    # Position loss with absolute error\n",
    "    replay_loss = pos_criterion(pos_outputs, pos_targets)  # Shape: (batch_size, 1, 2)\n",
    "    \n",
    "    # Sum the loss along the last dimension and compute the mean over the batch\n",
    "    replay_loss_mean = replay_loss.sum(dim=2).mean()\n",
    "    \n",
    "    # Object position (batch_size, 2)\n",
    "    object_pos = train_objects[:, :2]  # Shape: (batch_size, 2)\n",
    "    object_pos = object_pos.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
    "    \n",
    "    # Time values (batch_size,)\n",
    "    time_values = torch.abs(train_objects[:, 2])\n",
    "    \n",
    "    # Valid types (hit circles or slider start)\n",
    "    type_weight = train_objects[:, 3]\n",
    "    \n",
    "    # Compute weights: weight increases from 0 to 1 as time decreases from 0.05 to 0\n",
    "    time_weights = torch.clamp((0.1 - time_values) / 0.1, min=0, max=1)  # Shape: (batch_size,)\n",
    "    \n",
    "    # Compute loss between pos_outputs and object_pos\n",
    "    object_loss = pos_criterion(pos_outputs, object_pos)  # Shape: (batch_size, 1, 2)\n",
    "    object_loss = object_loss.sum(dim=2).squeeze(1)  # Shape: (batch_size,)\n",
    "    \n",
    "    # r = 54.4 - 4.48 * CS\n",
    "    # Radius of hit circle formula from osu wiki\n",
    "    # Apply object loss for predictions outside of immediate object\n",
    "    # CS will be the maximum (10) to ensure the model is as accurate as possible which is 9.6 osu pixels\n",
    "    # epsilon_x = 9.6 / 512  # 0.01875\n",
    "    # epsilon_y = 9.6 / 384  # 0.025\n",
    "    # so epsilon will be epsilon_x + epsilon_y\n",
    "    # epsilon = 0.04375\n",
    "    \n",
    "    # Dynamic epsilon based on time\n",
    "    epsilon_min = 0.025\n",
    "    epsilon_max = 1\n",
    "    \n",
    "    epsilon = epsilon_min + (epsilon_max - epsilon_min) * torch.pow(time_values, 1.5)\n",
    "    adjusted_object_loss = torch.clamp(object_loss - epsilon, min=0)\n",
    "    \n",
    "    # Apply weights\n",
    "    weighted_object_loss = adjusted_object_loss * type_weight * time_weights  # Shape: (batch_size,)\n",
    "    \n",
    "    # Compute mean over the batch\n",
    "    object_loss_mean = weighted_object_loss.mean()\n",
    "    \n",
    "    return replay_loss_mean, object_loss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000]T Loss Replay: 0.0698 | T Loss Object: 0.0705 | V Loss Pos: 0.0486 | LR Pos: 0.01 | \n",
      "Epoch [2/1000]T Loss Replay: 0.0368 | T Loss Object: 0.0091 | V Loss Pos: 0.0359 | LR Pos: 0.01 | \n",
      "Epoch [3/1000]T Loss Replay: 0.0305 | T Loss Object: 0.0084 | V Loss Pos: 0.0321 | LR Pos: 0.01 | \n",
      "Epoch [4/1000]T Loss Replay: 0.0270 | T Loss Object: 0.0085 | V Loss Pos: 0.0326 | LR Pos: 0.01 | \n",
      "Epoch [5/1000]T Loss Replay: 0.0280 | T Loss Object: 0.0089 | V Loss Pos: 0.0373 | LR Pos: 0.01 | \n",
      "Epoch [6/1000]T Loss Replay: 0.0257 | T Loss Object: 0.0089 | V Loss Pos: 0.0301 | LR Pos: 0.01 | \n",
      "Epoch [7/1000]T Loss Replay: 0.0249 | T Loss Object: 0.0089 | V Loss Pos: 0.0314 | LR Pos: 0.01 | \n",
      "Epoch [8/1000]T Loss Replay: 0.0240 | T Loss Object: 0.0089 | V Loss Pos: 0.0280 | LR Pos: 0.01 | \n",
      "Epoch [9/1000]T Loss Replay: 0.0244 | T Loss Object: 0.0090 | V Loss Pos: 0.0276 | LR Pos: 0.01 | \n",
      "Epoch [10/1000]T Loss Replay: 0.0231 | T Loss Object: 0.0089 | V Loss Pos: 0.0268 | LR Pos: 0.01 | \n",
      "Epoch [11/1000]T Loss Replay: 0.0227 | T Loss Object: 0.0089 | V Loss Pos: 0.0256 | LR Pos: 0.01 | \n",
      "Epoch [12/1000]T Loss Replay: 0.0236 | T Loss Object: 0.0089 | V Loss Pos: 0.0259 | LR Pos: 0.01 | \n",
      "Epoch [13/1000]T Loss Replay: 0.0225 | T Loss Object: 0.0088 | V Loss Pos: 0.0255 | LR Pos: 0.01 | \n",
      "Epoch [14/1000]T Loss Replay: 0.0225 | T Loss Object: 0.0088 | V Loss Pos: 0.0250 | LR Pos: 0.01 | \n",
      "Epoch [15/1000]T Loss Replay: 0.0223 | T Loss Object: 0.0087 | V Loss Pos: 0.0267 | LR Pos: 0.01 | \n",
      "Epoch [16/1000]T Loss Replay: 0.0223 | T Loss Object: 0.0087 | V Loss Pos: 0.0266 | LR Pos: 0.01 | \n",
      "Epoch [17/1000]T Loss Replay: 0.0223 | T Loss Object: 0.0086 | V Loss Pos: 0.0266 | LR Pos: 0.01 | \n",
      "Epoch [18/1000]T Loss Replay: 0.0221 | T Loss Object: 0.0086 | V Loss Pos: 0.0270 | LR Pos: 0.01 | \n",
      "Epoch [19/1000]T Loss Replay: 0.0223 | T Loss Object: 0.0085 | V Loss Pos: 0.0265 | LR Pos: 0.01 | \n",
      "Epoch [20/1000]T Loss Replay: 0.0222 | T Loss Object: 0.0085 | V Loss Pos: 0.0291 | LR Pos: 0.01 | \n",
      "Epoch [21/1000]T Loss Replay: 0.0222 | T Loss Object: 0.0085 | V Loss Pos: 0.0283 | LR Pos: 0.01 | \n",
      "Epoch [22/1000]T Loss Replay: 0.0221 | T Loss Object: 0.0085 | V Loss Pos: 0.0251 | LR Pos: 0.01 | \n",
      "Epoch [23/1000]T Loss Replay: 0.0223 | T Loss Object: 0.0085 | V Loss Pos: 0.0266 | LR Pos: 0.01 | \n",
      "Epoch [24/1000]T Loss Replay: 0.0221 | T Loss Object: 0.0085 | V Loss Pos: 0.0260 | LR Pos: 0.01 | \n",
      "Epoch [25/1000]T Loss Replay: 0.0224 | T Loss Object: 0.0085 | V Loss Pos: 0.0260 | LR Pos: 0.01 | \n",
      "Epoch [26/1000]T Loss Replay: 0.0218 | T Loss Object: 0.0085 | V Loss Pos: 0.0285 | LR Pos: 0.01 | \n",
      "Epoch [27/1000]T Loss Replay: 0.0220 | T Loss Object: 0.0084 | V Loss Pos: 0.0274 | LR Pos: 0.01 | \n",
      "Epoch [28/1000]T Loss Replay: 0.0221 | T Loss Object: 0.0085 | V Loss Pos: 0.0262 | LR Pos: 0.01 | \n",
      "Epoch [29/1000]T Loss Replay: 0.0225 | T Loss Object: 0.0085 | V Loss Pos: 0.0282 | LR Pos: 0.01 | \n",
      "Epoch [30/1000]T Loss Replay: 0.0218 | T Loss Object: 0.0085 | V Loss Pos: 0.0256 | LR Pos: 0.001 | \n",
      "Epoch [31/1000]T Loss Replay: 0.0204 | T Loss Object: 0.0084 | V Loss Pos: 0.0246 | LR Pos: 0.001 | \n",
      "Epoch [32/1000]T Loss Replay: 0.0204 | T Loss Object: 0.0083 | V Loss Pos: 0.0249 | LR Pos: 0.001 | \n",
      "Epoch [33/1000]T Loss Replay: 0.0204 | T Loss Object: 0.0083 | V Loss Pos: 0.0248 | LR Pos: 0.001 | \n",
      "Epoch [34/1000]T Loss Replay: 0.0204 | T Loss Object: 0.0083 | V Loss Pos: 0.0251 | LR Pos: 0.001 | \n",
      "Epoch [35/1000]T Loss Replay: 0.0205 | T Loss Object: 0.0083 | V Loss Pos: 0.0249 | LR Pos: 0.001 | \n",
      "Epoch [36/1000]T Loss Replay: 0.0205 | T Loss Object: 0.0083 | V Loss Pos: 0.0251 | LR Pos: 0.001 | \n",
      "Epoch [37/1000]T Loss Replay: 0.0205 | T Loss Object: 0.0082 | V Loss Pos: 0.0243 | LR Pos: 0.001 | \n",
      "Epoch [38/1000]T Loss Replay: 0.0205 | T Loss Object: 0.0082 | V Loss Pos: 0.0247 | LR Pos: 0.001 | \n",
      "Epoch [39/1000]T Loss Replay: 0.0205 | T Loss Object: 0.0082 | V Loss Pos: 0.0247 | LR Pos: 0.001 | \n",
      "Epoch [40/1000]T Loss Replay: 0.0205 | T Loss Object: 0.0082 | V Loss Pos: 0.0245 | LR Pos: 0.001 | \n",
      "Epoch [41/1000]T Loss Replay: 0.0205 | T Loss Object: 0.0082 | V Loss Pos: 0.0247 | LR Pos: 0.001 | \n",
      "Epoch [42/1000]T Loss Replay: 0.0205 | T Loss Object: 0.0082 | V Loss Pos: 0.0251 | LR Pos: 0.001 | \n",
      "Epoch [43/1000]T Loss Replay: 0.0205 | T Loss Object: 0.0082 | V Loss Pos: 0.0247 | LR Pos: 0.001 | \n",
      "Epoch [44/1000]T Loss Replay: 0.0205 | T Loss Object: 0.0082 | V Loss Pos: 0.0246 | LR Pos: 0.001 | \n",
      "Epoch [45/1000]T Loss Replay: 0.0205 | T Loss Object: 0.0082 | V Loss Pos: 0.0246 | LR Pos: 0.001 | \n",
      "Epoch [46/1000]T Loss Replay: 0.0205 | T Loss Object: 0.0082 | V Loss Pos: 0.0248 | LR Pos: 0.001 | \n",
      "Epoch [47/1000]T Loss Replay: 0.0205 | T Loss Object: 0.0082 | V Loss Pos: 0.0247 | LR Pos: 0.001 | \n",
      "Epoch [48/1000]T Loss Replay: 0.0205 | T Loss Object: 0.0082 | V Loss Pos: 0.0246 | LR Pos: 0.001 | \n",
      "Epoch [49/1000]T Loss Replay: 0.0205 | T Loss Object: 0.0081 | V Loss Pos: 0.0246 | LR Pos: 0.001 | \n",
      "Epoch [50/1000]T Loss Replay: 0.0205 | T Loss Object: 0.0081 | V Loss Pos: 0.0253 | LR Pos: 0.001 | \n",
      "Epoch [51/1000]T Loss Replay: 0.0205 | T Loss Object: 0.0081 | V Loss Pos: 0.0249 | LR Pos: 0.001 | \n",
      "Epoch [52/1000]T Loss Replay: 0.0205 | T Loss Object: 0.0081 | V Loss Pos: 0.0253 | LR Pos: 0.001 | \n",
      "Epoch [53/1000]T Loss Replay: 0.0205 | T Loss Object: 0.0081 | V Loss Pos: 0.0252 | LR Pos: 0.0001 | \n",
      "Epoch [54/1000]T Loss Replay: 0.0203 | T Loss Object: 0.0081 | V Loss Pos: 0.0247 | LR Pos: 0.0001 | \n",
      "Epoch [55/1000]T Loss Replay: 0.0203 | T Loss Object: 0.0081 | V Loss Pos: 0.0247 | LR Pos: 0.0001 | \n",
      "Epoch [56/1000]T Loss Replay: 0.0203 | T Loss Object: 0.0081 | V Loss Pos: 0.0247 | LR Pos: 0.0001 | \n",
      "Epoch [57/1000]T Loss Replay: 0.0203 | T Loss Object: 0.0081 | V Loss Pos: 0.0246 | LR Pos: 0.0001 | \n",
      "Epoch [58/1000]T Loss Replay: 0.0203 | T Loss Object: 0.0081 | V Loss Pos: 0.0247 | LR Pos: 0.0001 | \n",
      "Epoch [59/1000]T Loss Replay: 0.0203 | T Loss Object: 0.0081 | V Loss Pos: 0.0249 | LR Pos: 0.0001 | \n",
      "Epoch [60/1000]T Loss Replay: 0.0203 | T Loss Object: 0.0081 | V Loss Pos: 0.0247 | LR Pos: 0.0001 | \n",
      "Epoch [61/1000]T Loss Replay: 0.0203 | T Loss Object: 0.0081 | V Loss Pos: 0.0248 | LR Pos: 0.0001 | \n",
      "Epoch [62/1000]T Loss Replay: 0.0203 | T Loss Object: 0.0081 | V Loss Pos: 0.0248 | LR Pos: 0.0001 | \n",
      "Epoch [63/1000]T Loss Replay: 0.0203 | T Loss Object: 0.0081 | V Loss Pos: 0.0248 | LR Pos: 0.0001 | \n",
      "Epoch [64/1000]T Loss Replay: 0.0203 | T Loss Object: 0.0081 | V Loss Pos: 0.0246 | LR Pos: 0.0001 | \n",
      "Epoch [65/1000]T Loss Replay: 0.0203 | T Loss Object: 0.0081 | V Loss Pos: 0.0245 | LR Pos: 0.0001 | \n",
      "Epoch [66/1000]T Loss Replay: 0.0203 | T Loss Object: 0.0081 | V Loss Pos: 0.0245 | LR Pos: 0.0001 | \n",
      "Epoch [67/1000]T Loss Replay: 0.0203 | T Loss Object: 0.0081 | V Loss Pos: 0.0247 | LR Pos: 0.0001 | \n",
      "Epoch [68/1000]T Loss Replay: 0.0203 | T Loss Object: 0.0081 | V Loss Pos: 0.0247 | LR Pos: 0.0001 | \n",
      "Epoch [69/1000]T Loss Replay: 0.0203 | T Loss Object: 0.0081 | V Loss Pos: 0.0247 | LR Pos: 1e-05 | \n",
      "Epoch [70/1000]T Loss Replay: 0.0203 | T Loss Object: 0.0081 | V Loss Pos: 0.0247 | LR Pos: 1e-05 | \n",
      "Epoch [71/1000]T Loss Replay: 0.0202 | T Loss Object: 0.0081 | V Loss Pos: 0.0247 | LR Pos: 1e-05 | \n",
      "Epoch [72/1000]T Loss Replay: 0.0202 | T Loss Object: 0.0081 | V Loss Pos: 0.0247 | LR Pos: 1e-05 | \n",
      "Epoch [73/1000]T Loss Replay: 0.0202 | T Loss Object: 0.0081 | V Loss Pos: 0.0247 | LR Pos: 1e-05 | \n",
      "Epoch [74/1000]T Loss Replay: 0.0202 | T Loss Object: 0.0081 | V Loss Pos: 0.0247 | LR Pos: 1e-05 | \n",
      "Epoch [75/1000]T Loss Replay: 0.0202 | T Loss Object: 0.0081 | V Loss Pos: 0.0247 | LR Pos: 1e-05 | \n",
      "Epoch [76/1000]T Loss Replay: 0.0202 | T Loss Object: 0.0081 | V Loss Pos: 0.0247 | LR Pos: 1e-05 | \n",
      "Epoch [77/1000]T Loss Replay: 0.0202 | T Loss Object: 0.0081 | V Loss Pos: 0.0247 | LR Pos: 1e-05 | \n",
      "Epoch [78/1000]T Loss Replay: 0.0202 | T Loss Object: 0.0081 | V Loss Pos: 0.0248 | LR Pos: 1e-05 | \n",
      "Epoch [79/1000]T Loss Replay: 0.0202 | T Loss Object: 0.0081 | V Loss Pos: 0.0248 | LR Pos: 1e-05 | \n",
      "Epoch [80/1000]T Loss Replay: 0.0202 | T Loss Object: 0.0081 | V Loss Pos: 0.0247 | LR Pos: 1e-05 | \n",
      "Epoch [81/1000]T Loss Replay: 0.0202 | T Loss Object: 0.0081 | V Loss Pos: 0.0247 | LR Pos: 1e-05 | \n",
      "Epoch [82/1000]T Loss Replay: 0.0202 | T Loss Object: 0.0081 | V Loss Pos: 0.0247 | LR Pos: 1e-05 | \n",
      "Epoch [83/1000]T Loss Replay: 0.0202 | T Loss Object: 0.0081 | V Loss Pos: 0.0247 | LR Pos: 1e-05 | \n",
      "Epoch [84/1000]T Loss Replay: 0.0202 | T Loss Object: 0.0081 | V Loss Pos: 0.0247 | LR Pos: 1e-05 | \n",
      "Epoch [85/1000]T Loss Replay: 0.0202 | T Loss Object: 0.0081 | V Loss Pos: 0.0247 | LR Pos: 1.0000000000000002e-06 | \n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 1000\n",
    "total_train_len = len(train_loader)\n",
    "total_valid_len = 0\n",
    "for valid_loader in valid_loaders:\n",
    "    total_valid_len += len(valid_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    osuModelPos.train()\n",
    "    train_loss_replay = 0\n",
    "    train_loss_object = 0\n",
    "    \n",
    "    # Training\n",
    "    # for train_inputs, train_targets, input_lengths, target_lengths in train_loader:\n",
    "    for train_inputs, train_targets, input_lengths, train_objects in train_loader:\n",
    "        train_inputs, train_targets, train_objects = train_inputs.to(device), train_targets.to(device), train_objects.to(device)\n",
    "        \n",
    "        pos_optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagate\n",
    "        train_pos_outputs = osuModelPos(train_inputs, train_targets)\n",
    "        \n",
    "        # Compute losses\n",
    "        loss_replay, loss_object = get_hybrid_loss(train_pos_outputs, train_targets, train_objects)\n",
    "        # loss_pos = get_pos_loss(train_pos_outputs, train_targets)\n",
    "        \n",
    "        # Accumulate losses\n",
    "        train_loss_replay += loss_replay.item()\n",
    "        train_loss_object += loss_object.item()\n",
    "        \n",
    "        # Position encoder decoder back propagation\n",
    "        loss_pos = loss_replay + loss_object\n",
    "        loss_pos.backward()\n",
    "        \n",
    "        # Gradient clipping \n",
    "        torch.nn.utils.clip_grad_norm_(osuModelPos.parameters(), max_norm=1.0)\n",
    "        # Updating weights\n",
    "        pos_optimizer.step()\n",
    "        \n",
    "    train_loss_replay /= total_train_len\n",
    "    train_loss_object /= total_train_len\n",
    "    \n",
    "    # Validation\n",
    "    osuModelPos.eval()\n",
    "    total_valid_loss_pos = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for valid_loader in valid_loaders:\n",
    "            valid_loss_pos = 0\n",
    "            valid_loss_key = 0\n",
    "            # for valid_inputs, valid_targets, input_lengths, target_lengths in valid_loader:\n",
    "            for valid_inputs, valid_targets, input_lengths, valid_objects in valid_loader:\n",
    "                valid_inputs, valid_targets = valid_inputs.to(device), valid_targets.to(device)\n",
    "\n",
    "                valid_pos_outputs = osuModelPos(valid_inputs, valid_targets)\n",
    "                \n",
    "                # loss_pos = get_weighted_pos_loss(valid_pos_outputs, valid_targets, valid_inputs)\n",
    "                loss_pos = get_pos_loss(valid_pos_outputs, valid_targets)\n",
    "\n",
    "                valid_loss_pos += loss_pos.item()\n",
    "                \n",
    "            total_valid_loss_pos += valid_loss_pos\n",
    "            \n",
    "    total_valid_loss_pos /= total_valid_len\n",
    "    \n",
    "    pos_scheduler.step(total_valid_loss_pos)\n",
    "    \n",
    "    pos_lr = pos_optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Printing info\n",
    "    print(\n",
    "    f\"Epoch [{epoch+1}/{num_epochs}]\"\n",
    "    f\"T Loss Replay: {train_loss_replay:.4f} | \"\n",
    "    f\"T Loss Object: {train_loss_object:.4f} | \"\n",
    "    f\"V Loss Pos: {total_valid_loss_pos:.4f} | \"\n",
    "    f\"LR Pos: {pos_lr} | \"\n",
    "    # f\"TFR: {teacher_forcing_ratio:.2f}\"\n",
    "    )\n",
    "    \n",
    "    if pos_lr < 1e-5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = '0.12'\n",
    "torch.save(osuModelPos.state_dict(), f'./OSU_model_pos_{VERSION}.pth')     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "osu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
