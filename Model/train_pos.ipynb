{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Debug\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# import sys\n",
    "# from pathlib import Path\n",
    "# sys.path.append(str(Path.cwd().parent))\n",
    "# from utils.DatasetGenerator import get_X, get_y, get_pos_sequences, read_sequences\n",
    "# path = '../Data/train_pos/annoy'\n",
    "# df_y_pos, hr = get_y(path, 'pos')\n",
    "# df_X = get_X(path, hr)\n",
    "# get_pos_sequences([df_X, df_y_pos, path], 10, 500)\n",
    "# input_seq, target_seq = read_sequences(path, 'pos')\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # debug\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# import sys\n",
    "# from pathlib import Path\n",
    "# sys.path.append(str(Path.cwd().parent))\n",
    "# from utils.Visualizer import visualize\n",
    "# from utils.DataParser import parse_map, parse_replay_pos\n",
    "# path = '../Data/train_pos/shink/'\n",
    "# replay_path = list(Path(path).glob('*.osr'))[0]\n",
    "# map_path = list(Path(path).glob('*.osu'))[0]\n",
    "# replay_data, hr = parse_replay_pos(replay_path)\n",
    "# hit_objects, _ = parse_map(map_path, hr)\n",
    "# song = list(Path(path).glob('audio.*'))[0]\n",
    "# visualize(hit_objects, replay_data, song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "%load_ext autoreload\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "data_paths = {\n",
    "    'train': '../Data/train_pos',\n",
    "    'valid': '../Data/valid_pos'\n",
    "}\n",
    "REGENERATE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting train data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:04<00:00,  8.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting valid data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.91it/s]\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "# Data parsing\n",
    "from utils.DatasetGenerator import get_set\n",
    "datasets = {\n",
    "    'train': None,\n",
    "    'valid': None\n",
    "}\n",
    "\n",
    "for idx, set in enumerate(data_paths):\n",
    "    print(f'Converting {set} data')\n",
    "    data = get_set(data_paths[set], 'pos', REGENERATE)\n",
    "    \n",
    "    datasets[set] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating train sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:08<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating valid sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving train sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:33<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving valid sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# Sequence generation\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\".*weights_only=False.*\")\n",
    "from utils.DatasetGenerator import create_set_sequences, get_set_sequences\n",
    "from itertools import chain\n",
    "\n",
    "N = 3\n",
    "window = 400\n",
    "for idx, set in enumerate(datasets):\n",
    "    if len(datasets[set]) > 0:\n",
    "        print(f'Generating {set} sequences')\n",
    "        create_set_sequences(datasets[set], N, window, 'pos')\n",
    "\n",
    "combined_train_key_input = None\n",
    "combined_train_key_target = None\n",
    "valid_sequences = None\n",
    "for idx, set in enumerate(data_paths):\n",
    "    print(f'Retrieving {set} sequences')\n",
    "    key_input_sequences, key_target_sequences = get_set_sequences(data_paths[set], 'pos')\n",
    "    if set == 'train':\n",
    "        combined_train_key_input = list(chain.from_iterable(key_input_sequences))\n",
    "        combined_train_key_target = list(chain.from_iterable(key_target_sequences))\n",
    "    else:\n",
    "        valid_sequences = [key_input_sequences, key_target_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from Model.OSUDataset import OsuDataset, pos_collate_fn\n",
    "data_paths = {\n",
    "    'train': '../Data/train_pos',\n",
    "    'valid': '../Data/valid_pos'\n",
    "}\n",
    "BATCH_SIZE_TRAIN = 1024\n",
    "if os.path.exists('./pos_dataset/train_dataset.pth') and not REGENERATE:\n",
    "    train_dataset = torch.load('./pos_dataset/train_dataset.pth')\n",
    "else:\n",
    "    train_dataset = OsuDataset(combined_train_key_input, combined_train_key_target)\n",
    "    torch.save(train_dataset, './pos_dataset/train_dataset.pth')\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE_TRAIN, shuffle=True, drop_last=True, collate_fn=pos_collate_fn, pin_memory=True)\n",
    "\n",
    "BATCH_SIZE_VALID = 64\n",
    "valid_loaders = []\n",
    "valid_folders = os.listdir(data_paths['valid'])\n",
    "valid_data_size = sum(os.path.isdir(os.path.join(data_paths['valid'], folder)) for folder in valid_folders)\n",
    "for idx in range(valid_data_size):\n",
    "    if os.path.exists(f'./pos_dataset/valid_dataset_{idx}.pth') and not REGENERATE:\n",
    "        valid_dataset = torch.load(f'./pos_dataset/valid_dataset_{idx}.pth')\n",
    "    else:\n",
    "        valid_dataset = valid_dataset = OsuDataset(valid_sequences[0][idx], valid_sequences[1][idx])\n",
    "        torch.save(valid_dataset, f'./pos_dataset/valid_dataset_{idx}.pth')\n",
    "        \n",
    "    valid_loader =  DataLoader(valid_dataset, batch_size=BATCH_SIZE_VALID, shuffle=False, drop_last=True, collate_fn=pos_collate_fn, pin_memory=True)\n",
    "    valid_loaders.append(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from Model.OSUModel import PositionEncoder, PositionDecoder, OSUModelPos\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "pos_input_size = 10\n",
    "\n",
    "pos_hidden_size = 24\n",
    "pos_num_layers = 2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Encoders take (input_size, hidden_size, num_layers)\n",
    "pos_encoder = PositionEncoder(pos_input_size, pos_hidden_size, pos_num_layers).to(device)\n",
    "\n",
    "# Decoders takes (hidden_size, num_layers)\n",
    "pos_decoder = PositionDecoder(pos_hidden_size, pos_num_layers).to(device)\n",
    "\n",
    "osuModelPos = OSUModelPos(pos_encoder, pos_decoder, device).to(device)\n",
    "\n",
    "pos_criterion = nn.L1Loss(reduction='none')\n",
    "\n",
    "pos_optimizer = torch.optim.Adam(\n",
    "    osuModelPos.parameters(),\n",
    "    lr = 0.01,\n",
    "    weight_decay=1e-5\n",
    ")\n",
    "\n",
    "\n",
    "pos_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(pos_optimizer, mode='min',\n",
    "                                                    factor=0.1, patience=15,\n",
    "                                                    threshold=1e-4)\n",
    "\n",
    "  \n",
    "# def masked_loss_pos(pos_outputs, targets, target_lengths):\n",
    "#     # Position targets (batch_size, sequence_len - 1, 2)\n",
    "#     pos_targets = targets[:, 1:, :]\n",
    "\n",
    "#     # Creating mask for variable length sequences\n",
    "#     max_len = pos_outputs.size(1)\n",
    "#     #(batch_size, max_len). True when value is less than target length\n",
    "#     mask = torch.arange(max_len).unsqueeze(0) < target_lengths.unsqueeze(1) \n",
    "#     mask = mask.to(pos_outputs.device)\n",
    "\n",
    "#     # Position loss with absolute error.\n",
    "#     pos_loss = pos_criterion(pos_outputs, pos_targets)\n",
    "#     pos_loss = pos_loss.sum(dim=2)\n",
    "#     pos_loss = (pos_loss * mask).sum() / mask.sum()\n",
    "\n",
    "#     return pos_loss\n",
    "  \n",
    "def get_pos_loss(pos_outputs, targets):\n",
    "    # Position targets (batch_size, 2, 2)\n",
    "    # Removing previous positions from targets\n",
    "    pos_targets = targets[:, 1:, :]  \n",
    "\n",
    "    # Position loss with absolute error\n",
    "    pos_loss = pos_criterion(pos_outputs, pos_targets)\n",
    "    \n",
    "    # Sum the loss along the last dimension and compute the mean over the batch\n",
    "    pos_loss = pos_loss.sum(dim=2).mean()\n",
    "\n",
    "    return pos_loss\n",
    "\n",
    "def get_weighted_pos_loss(pos_outputs, targets, pos_inputs):\n",
    "    # Position targets (batch_size, 2, 2)\n",
    "    # Removing previous positions from targets\n",
    "    pos_targets = targets[:, 1:, :]  # Shape: (batch_size, 1, 2)\n",
    "    \n",
    "    # Position loss with absolute error (no reduction)\n",
    "    pos_loss = pos_criterion(pos_outputs, pos_targets)  # Shape: (batch_size, 1, 2)\n",
    "        \n",
    "    # Sum the loss along the last dimension to get per-sample loss\n",
    "    pos_loss = pos_loss.sum(dim=2).squeeze(1)  # Shape: (batch_size,)\n",
    "    \n",
    "    inputs_padded, lengths = pad_packed_sequence(pos_inputs, batch_first=True)\n",
    "    \n",
    "    batch_size = inputs_padded.size(0)\n",
    "    \n",
    "    # Extract times and object_types for first two objects\n",
    "    times_padded = inputs_padded[:, :2, 7]  # Shape: (batch_size, 2)\n",
    "    object_types_padded = inputs_padded[:, :2, 2]  # Shape: (batch_size, 2)\n",
    "    \n",
    "    # Create valid_mask indicating valid entries (sequence lengths may be less than 2)\n",
    "    positions = torch.arange(2, device=pos_outputs.device).unsqueeze(0)  # Shape: (1, 2)\n",
    "    lengths_expanded = (lengths.unsqueeze(1)).to(pos_outputs.device)  # Shape: (batch_size, 1)\n",
    "    valid_mask = positions < lengths_expanded  # Shape: (batch_size, 2)\n",
    "    \n",
    "    # Set times to a large value where entries are invalid (so they won't be selected as minimum)\n",
    "    times_padded = times_padded.clone()\n",
    "    times_padded[~valid_mask] = float('inf')\n",
    "    \n",
    "    # Compute absolute times\n",
    "    abs_times_padded = times_padded.abs()  # Shape: (batch_size, 2)\n",
    "    \n",
    "    # Get indices of minimum times\n",
    "    min_time_indices = abs_times_padded.argmin(dim=1)  # Shape: (batch_size,)\n",
    "    \n",
    "    # Gather the times and object_types corresponding to min_time_indices\n",
    "    times = times_padded[torch.arange(batch_size), min_time_indices]  # Shape: (batch_size,)\n",
    "    object_types = object_types_padded[torch.arange(batch_size), min_time_indices]  # Shape: (batch_size,)\n",
    "    \n",
    "    # Define the time threshold and extra weight\n",
    "    time_threshold = 0.2  # Adjust this threshold as needed\n",
    "    extra_weight = 3.0    # Adjust the extra weight as needed\n",
    "    \n",
    "    # Compute a smooth weight based on time\n",
    "    time_weight = ((time_threshold - times).clamp(min=0.0) / time_threshold).clamp(max=1.0)\n",
    "    # Multiply by object type indicator (1 if type 1, 0 otherwise)\n",
    "    weights = 1.0 + time_weight * (object_types == 1).float() * (extra_weight - 1.0)\n",
    "    \n",
    "    # Apply the weights to the per-sample losses\n",
    "    weighted_loss = pos_loss * weights  # Shape: (batch_size,)\n",
    "    \n",
    "    # Compute the mean over the batch\n",
    "    pos_loss = weighted_loss.mean()\n",
    "    \n",
    "    return pos_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000]T Loss Pos: 0.0832 | V Loss Pos: 0.0422 | LR Pos: 0.01 | TFR: 1.00\n",
      "Epoch [2/1000]T Loss Pos: 0.0337 | V Loss Pos: 0.0453 | LR Pos: 0.01 | TFR: 0.95\n",
      "Epoch [3/1000]T Loss Pos: 0.0265 | V Loss Pos: 0.0298 | LR Pos: 0.01 | TFR: 0.90\n",
      "Epoch [4/1000]T Loss Pos: 0.0234 | V Loss Pos: 0.0277 | LR Pos: 0.01 | TFR: 0.85\n",
      "Epoch [5/1000]T Loss Pos: 0.0238 | V Loss Pos: 0.0243 | LR Pos: 0.01 | TFR: 0.80\n",
      "Epoch [6/1000]T Loss Pos: 0.0225 | V Loss Pos: 0.0239 | LR Pos: 0.01 | TFR: 0.75\n",
      "Epoch [7/1000]T Loss Pos: 0.0223 | V Loss Pos: 0.0237 | LR Pos: 0.01 | TFR: 0.70\n",
      "Epoch [8/1000]T Loss Pos: 0.0214 | V Loss Pos: 0.0230 | LR Pos: 0.01 | TFR: 0.65\n",
      "Epoch [9/1000]T Loss Pos: 0.0207 | V Loss Pos: 0.0235 | LR Pos: 0.01 | TFR: 0.60\n",
      "Epoch [10/1000]T Loss Pos: 0.0219 | V Loss Pos: 0.0237 | LR Pos: 0.01 | TFR: 0.55\n",
      "Epoch [11/1000]T Loss Pos: 0.0201 | V Loss Pos: 0.0257 | LR Pos: 0.01 | TFR: 0.50\n",
      "Epoch [12/1000]T Loss Pos: 0.0200 | V Loss Pos: 0.0226 | LR Pos: 0.01 | TFR: 0.45\n",
      "Epoch [13/1000]T Loss Pos: 0.0200 | V Loss Pos: 0.0222 | LR Pos: 0.01 | TFR: 0.40\n",
      "Epoch [14/1000]T Loss Pos: 0.0202 | V Loss Pos: 0.0257 | LR Pos: 0.01 | TFR: 0.35\n",
      "Epoch [15/1000]T Loss Pos: 0.0195 | V Loss Pos: 0.0231 | LR Pos: 0.01 | TFR: 0.30\n",
      "Epoch [16/1000]T Loss Pos: 0.0218 | V Loss Pos: 0.0227 | LR Pos: 0.01 | TFR: 0.25\n",
      "Epoch [17/1000]T Loss Pos: 0.0189 | V Loss Pos: 0.0219 | LR Pos: 0.01 | TFR: 0.20\n",
      "Epoch [18/1000]T Loss Pos: 0.0196 | V Loss Pos: 0.0254 | LR Pos: 0.01 | TFR: 0.15\n",
      "Epoch [19/1000]T Loss Pos: 0.0188 | V Loss Pos: 0.0219 | LR Pos: 0.01 | TFR: 0.10\n",
      "Epoch [20/1000]T Loss Pos: 0.0202 | V Loss Pos: 0.0232 | LR Pos: 0.01 | TFR: 0.05\n",
      "Epoch [21/1000]T Loss Pos: 0.0188 | V Loss Pos: 0.0216 | LR Pos: 0.01 | TFR: 0.00\n",
      "Epoch [22/1000]T Loss Pos: 0.0189 | V Loss Pos: 0.0231 | LR Pos: 0.01 | TFR: 0.00\n",
      "Epoch [23/1000]T Loss Pos: 0.0190 | V Loss Pos: 0.0225 | LR Pos: 0.01 | TFR: 0.00\n",
      "Epoch [24/1000]T Loss Pos: 0.0187 | V Loss Pos: 0.0231 | LR Pos: 0.01 | TFR: 0.00\n",
      "Epoch [25/1000]T Loss Pos: 0.0188 | V Loss Pos: 0.0221 | LR Pos: 0.01 | TFR: 0.00\n",
      "Epoch [26/1000]T Loss Pos: 0.0188 | V Loss Pos: 0.0222 | LR Pos: 0.01 | TFR: 0.00\n",
      "Epoch [27/1000]T Loss Pos: 0.0188 | V Loss Pos: 0.0225 | LR Pos: 0.01 | TFR: 0.00\n",
      "Epoch [28/1000]T Loss Pos: 0.0191 | V Loss Pos: 0.0219 | LR Pos: 0.01 | TFR: 0.00\n",
      "Epoch [29/1000]T Loss Pos: 0.0185 | V Loss Pos: 0.0225 | LR Pos: 0.01 | TFR: 0.00\n",
      "Epoch [30/1000]T Loss Pos: 0.0187 | V Loss Pos: 0.0223 | LR Pos: 0.01 | TFR: 0.00\n",
      "Epoch [31/1000]T Loss Pos: 0.0188 | V Loss Pos: 0.0216 | LR Pos: 0.01 | TFR: 0.00\n",
      "Epoch [32/1000]T Loss Pos: 0.0184 | V Loss Pos: 0.0216 | LR Pos: 0.01 | TFR: 0.00\n",
      "Epoch [33/1000]T Loss Pos: 0.0186 | V Loss Pos: 0.0216 | LR Pos: 0.01 | TFR: 0.00\n",
      "Epoch [34/1000]T Loss Pos: 0.0186 | V Loss Pos: 0.0231 | LR Pos: 0.01 | TFR: 0.00\n",
      "Epoch [35/1000]T Loss Pos: 0.0188 | V Loss Pos: 0.0228 | LR Pos: 0.01 | TFR: 0.00\n",
      "Epoch [36/1000]T Loss Pos: 0.0184 | V Loss Pos: 0.0216 | LR Pos: 0.01 | TFR: 0.00\n",
      "Epoch [37/1000]T Loss Pos: 0.0186 | V Loss Pos: 0.0222 | LR Pos: 0.01 | TFR: 0.00\n",
      "Epoch [38/1000]T Loss Pos: 0.0185 | V Loss Pos: 0.0220 | LR Pos: 0.01 | TFR: 0.00\n",
      "Epoch [39/1000]T Loss Pos: 0.0184 | V Loss Pos: 0.0229 | LR Pos: 0.01 | TFR: 0.00\n",
      "Epoch [40/1000]T Loss Pos: 0.0188 | V Loss Pos: 0.0257 | LR Pos: 0.01 | TFR: 0.00\n",
      "Epoch [41/1000]T Loss Pos: 0.0187 | V Loss Pos: 0.0241 | LR Pos: 0.01 | TFR: 0.00\n",
      "Epoch [42/1000]T Loss Pos: 0.0186 | V Loss Pos: 0.0223 | LR Pos: 0.01 | TFR: 0.00\n",
      "Epoch [43/1000]T Loss Pos: 0.0183 | V Loss Pos: 0.0220 | LR Pos: 0.01 | TFR: 0.00\n",
      "Epoch [44/1000]T Loss Pos: 0.0184 | V Loss Pos: 0.0221 | LR Pos: 0.01 | TFR: 0.00\n",
      "Epoch [45/1000]T Loss Pos: 0.0185 | V Loss Pos: 0.0224 | LR Pos: 0.01 | TFR: 0.00\n",
      "Epoch [46/1000]T Loss Pos: 0.0186 | V Loss Pos: 0.0220 | LR Pos: 0.01 | TFR: 0.00\n",
      "Epoch [47/1000]T Loss Pos: 0.0184 | V Loss Pos: 0.0219 | LR Pos: 0.01 | TFR: 0.00\n",
      "Epoch [48/1000]T Loss Pos: 0.0184 | V Loss Pos: 0.0219 | LR Pos: 0.01 | TFR: 0.00\n",
      "Epoch [49/1000]T Loss Pos: 0.0183 | V Loss Pos: 0.0222 | LR Pos: 0.01 | TFR: 0.00\n",
      "Epoch [50/1000]T Loss Pos: 0.0184 | V Loss Pos: 0.0233 | LR Pos: 0.01 | TFR: 0.00\n",
      "Epoch [51/1000]T Loss Pos: 0.0183 | V Loss Pos: 0.0237 | LR Pos: 0.01 | TFR: 0.00\n",
      "Epoch [52/1000]T Loss Pos: 0.0183 | V Loss Pos: 0.0224 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [53/1000]T Loss Pos: 0.0171 | V Loss Pos: 0.0214 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [54/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0211 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [55/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0212 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [56/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0213 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [57/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0212 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [58/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0212 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [59/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0213 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [60/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0211 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [61/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0212 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [62/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0212 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [63/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0212 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [64/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0212 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [65/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0215 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [66/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0212 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [67/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0213 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [68/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0212 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [69/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0211 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [70/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0211 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [71/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0210 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [72/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0213 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [73/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0210 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [74/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0212 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [75/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0211 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [76/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0210 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [77/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0213 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [78/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0214 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [79/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0211 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [80/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0211 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [81/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0214 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [82/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0212 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [83/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0212 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [84/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0213 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [85/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0211 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [86/1000]T Loss Pos: 0.0169 | V Loss Pos: 0.0211 | LR Pos: 0.001 | TFR: 0.00\n",
      "Epoch [87/1000]T Loss Pos: 0.0170 | V Loss Pos: 0.0211 | LR Pos: 0.0001 | TFR: 0.00\n",
      "Epoch [88/1000]T Loss Pos: 0.0168 | V Loss Pos: 0.0210 | LR Pos: 0.0001 | TFR: 0.00\n",
      "Epoch [89/1000]T Loss Pos: 0.0168 | V Loss Pos: 0.0209 | LR Pos: 0.0001 | TFR: 0.00\n",
      "Epoch [90/1000]T Loss Pos: 0.0168 | V Loss Pos: 0.0209 | LR Pos: 0.0001 | TFR: 0.00\n",
      "Epoch [91/1000]T Loss Pos: 0.0168 | V Loss Pos: 0.0210 | LR Pos: 0.0001 | TFR: 0.00\n",
      "Epoch [92/1000]T Loss Pos: 0.0168 | V Loss Pos: 0.0209 | LR Pos: 0.0001 | TFR: 0.00\n",
      "Epoch [93/1000]T Loss Pos: 0.0168 | V Loss Pos: 0.0210 | LR Pos: 0.0001 | TFR: 0.00\n",
      "Epoch [94/1000]T Loss Pos: 0.0168 | V Loss Pos: 0.0210 | LR Pos: 0.0001 | TFR: 0.00\n",
      "Epoch [95/1000]T Loss Pos: 0.0168 | V Loss Pos: 0.0210 | LR Pos: 0.0001 | TFR: 0.00\n",
      "Epoch [96/1000]T Loss Pos: 0.0168 | V Loss Pos: 0.0210 | LR Pos: 0.0001 | TFR: 0.00\n",
      "Epoch [97/1000]T Loss Pos: 0.0168 | V Loss Pos: 0.0209 | LR Pos: 0.0001 | TFR: 0.00\n",
      "Epoch [98/1000]T Loss Pos: 0.0168 | V Loss Pos: 0.0210 | LR Pos: 0.0001 | TFR: 0.00\n",
      "Epoch [99/1000]T Loss Pos: 0.0168 | V Loss Pos: 0.0210 | LR Pos: 0.0001 | TFR: 0.00\n",
      "Epoch [100/1000]T Loss Pos: 0.0168 | V Loss Pos: 0.0210 | LR Pos: 0.0001 | TFR: 0.00\n",
      "Epoch [101/1000]T Loss Pos: 0.0168 | V Loss Pos: 0.0210 | LR Pos: 0.0001 | TFR: 0.00\n",
      "Epoch [102/1000]T Loss Pos: 0.0168 | V Loss Pos: 0.0209 | LR Pos: 0.0001 | TFR: 0.00\n",
      "Epoch [103/1000]T Loss Pos: 0.0168 | V Loss Pos: 0.0210 | LR Pos: 0.0001 | TFR: 0.00\n",
      "Epoch [104/1000]T Loss Pos: 0.0168 | V Loss Pos: 0.0211 | LR Pos: 0.0001 | TFR: 0.00\n",
      "Epoch [105/1000]T Loss Pos: 0.0168 | V Loss Pos: 0.0210 | LR Pos: 0.0001 | TFR: 0.00\n",
      "Epoch [106/1000]T Loss Pos: 0.0168 | V Loss Pos: 0.0210 | LR Pos: 0.0001 | TFR: 0.00\n",
      "Epoch [107/1000]T Loss Pos: 0.0168 | V Loss Pos: 0.0210 | LR Pos: 0.0001 | TFR: 0.00\n",
      "Epoch [108/1000]T Loss Pos: 0.0168 | V Loss Pos: 0.0211 | LR Pos: 0.0001 | TFR: 0.00\n",
      "Epoch [109/1000]T Loss Pos: 0.0168 | V Loss Pos: 0.0211 | LR Pos: 0.0001 | TFR: 0.00\n",
      "Epoch [110/1000]T Loss Pos: 0.0168 | V Loss Pos: 0.0210 | LR Pos: 0.0001 | TFR: 0.00\n",
      "Epoch [111/1000]T Loss Pos: 0.0168 | V Loss Pos: 0.0210 | LR Pos: 0.0001 | TFR: 0.00\n",
      "Epoch [112/1000]T Loss Pos: 0.0168 | V Loss Pos: 0.0210 | LR Pos: 0.0001 | TFR: 0.00\n",
      "Epoch [113/1000]T Loss Pos: 0.0168 | V Loss Pos: 0.0209 | LR Pos: 0.0001 | TFR: 0.00\n",
      "Epoch [114/1000]T Loss Pos: 0.0168 | V Loss Pos: 0.0210 | LR Pos: 0.0001 | TFR: 0.00\n",
      "Epoch [115/1000]T Loss Pos: 0.0168 | V Loss Pos: 0.0210 | LR Pos: 0.0001 | TFR: 0.00\n",
      "Epoch [116/1000]T Loss Pos: 0.0168 | V Loss Pos: 0.0210 | LR Pos: 0.0001 | TFR: 0.00\n",
      "Epoch [117/1000]T Loss Pos: 0.0168 | V Loss Pos: 0.0210 | LR Pos: 0.0001 | TFR: 0.00\n",
      "Epoch [118/1000]T Loss Pos: 0.0168 | V Loss Pos: 0.0210 | LR Pos: 1e-05 | TFR: 0.00\n",
      "Epoch [119/1000]T Loss Pos: 0.0167 | V Loss Pos: 0.0210 | LR Pos: 1e-05 | TFR: 0.00\n",
      "Epoch [120/1000]T Loss Pos: 0.0167 | V Loss Pos: 0.0210 | LR Pos: 1e-05 | TFR: 0.00\n",
      "Epoch [121/1000]T Loss Pos: 0.0167 | V Loss Pos: 0.0210 | LR Pos: 1e-05 | TFR: 0.00\n",
      "Epoch [122/1000]T Loss Pos: 0.0167 | V Loss Pos: 0.0210 | LR Pos: 1e-05 | TFR: 0.00\n",
      "Epoch [123/1000]T Loss Pos: 0.0167 | V Loss Pos: 0.0210 | LR Pos: 1e-05 | TFR: 0.00\n",
      "Epoch [124/1000]T Loss Pos: 0.0167 | V Loss Pos: 0.0210 | LR Pos: 1e-05 | TFR: 0.00\n",
      "Epoch [125/1000]T Loss Pos: 0.0167 | V Loss Pos: 0.0210 | LR Pos: 1e-05 | TFR: 0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m teacher_forcing_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m (epoch \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m20\u001b[39m), \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# for train_inputs, train_targets, input_lengths, target_lengths in train_loader:\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_targets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_inputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_targets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpos_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kq836\\source\\repos\\listener\\osu\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\kq836\\source\\repos\\listener\\osu\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\kq836\\source\\repos\\listener\\osu\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kq836\\source\\repos\\listener\\Model\\OSUDataset.py:24\u001b[0m, in \u001b[0;36mpos_collate_fn\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     21\u001b[0m inputs, targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Padding\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m input_lengths \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m inputs])\n\u001b[0;32m     25\u001b[0m padded_inputs \u001b[38;5;241m=\u001b[39m pad_sequence(inputs, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# target_lengths = torch.tensor([len(seq) for seq in targets])\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# padded_targets = pad_sequence(targets, batch_first=True, padding_value=0)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kq836\\source\\repos\\listener\\osu\\Lib\\site-packages\\torch\\_tensor.py:1017\u001b[0m, in \u001b[0;36mTensor.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;21m__neg__\u001b[39m \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39mTensorBase\u001b[38;5;241m.\u001b[39mneg\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;21m__abs__\u001b[39m \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39mTensorBase\u001b[38;5;241m.\u001b[39mabs\n\u001b[1;32m-> 1017\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1018\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1019\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__len__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "total_train_len = len(train_loader)\n",
    "total_valid_len = 0\n",
    "for valid_loader in valid_loaders:\n",
    "    total_valid_len += len(valid_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    osuModelPos.train()\n",
    "    train_loss_pos = 0\n",
    "    teacher_forcing_ratio = max(1 - (epoch / 20), 0)\n",
    "    \n",
    "    # Training\n",
    "    # for train_inputs, train_targets, input_lengths, target_lengths in train_loader:\n",
    "    for train_inputs, train_targets, input_lengths in train_loader:\n",
    "        train_inputs, train_targets = train_inputs.to(device), train_targets.to(device)\n",
    "        \n",
    "        pos_optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagate\n",
    "        train_pos_outputs = osuModelPos(train_inputs, train_targets, teacher_forcing_ratio)\n",
    "        \n",
    "        # Compute losses\n",
    "        # loss_pos = get_weighted_pos_loss(train_pos_outputs, train_targets, train_inputs)\n",
    "        loss_pos = get_pos_loss(train_pos_outputs, train_targets)\n",
    "        # loss_pos = masked_loss_pos(train_pos_outputs, train_targets, target_lengths)\n",
    "        \n",
    "        # Accumulate losses\n",
    "        train_loss_pos += loss_pos.item()\n",
    "        \n",
    "        # Position encoder decoder back propagation\n",
    "        loss_pos.backward()\n",
    "        \n",
    "        # Gradient clipping \n",
    "        torch.nn.utils.clip_grad_norm_(osuModelPos.parameters(), max_norm=1.0)\n",
    "        # Updating weights\n",
    "        pos_optimizer.step()\n",
    "        \n",
    "    train_loss_pos /= total_train_len\n",
    "    \n",
    "    # Validation\n",
    "    osuModelPos.eval()\n",
    "    total_valid_loss_pos = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for valid_loader in valid_loaders:\n",
    "            valid_loss_pos = 0\n",
    "            valid_loss_key = 0\n",
    "            # for valid_inputs, valid_targets, input_lengths, target_lengths in valid_loader:\n",
    "            for valid_inputs, valid_targets, input_lengths in valid_loader:\n",
    "                valid_inputs, valid_targets = valid_inputs.to(device), valid_targets.to(device)\n",
    "\n",
    "                valid_pos_outputs = osuModelPos(valid_inputs, valid_targets, 0)\n",
    "                \n",
    "                # loss_pos = get_weighted_pos_loss(valid_pos_outputs, valid_targets, valid_inputs)\n",
    "                loss_pos = get_pos_loss(valid_pos_outputs, valid_targets)\n",
    "                # loss_pos = masked_loss_pos(valid_pos_outputs, valid_targets, target_lengths)\n",
    "                valid_loss_pos += loss_pos.item()\n",
    "                \n",
    "            total_valid_loss_pos += valid_loss_pos\n",
    "            \n",
    "    total_valid_loss_pos /= total_valid_len\n",
    "    \n",
    "    pos_scheduler.step(total_valid_loss_pos)\n",
    "    \n",
    "    pos_lr = pos_optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Printing info\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\"\n",
    "    f\"T Loss Pos: {train_loss_pos:.4f} | \"\n",
    "    f\"V Loss Pos: {total_valid_loss_pos:.4f} | \"\n",
    "    f\"LR Pos: {pos_lr} | \"\n",
    "    f\"TFR: {teacher_forcing_ratio:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = '0.8'\n",
    "torch.save(osuModelPos.state_dict(), f'./OSU_model_pos_{VERSION}.pth')     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "osu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
